{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import torch\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimitris/miniconda3/envs/text/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/dimitris/miniconda3/envs/text/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/dimitris/miniconda3/envs/text/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(predicted_class=None, true_class=None):\n",
    "    if predicted_class is None or true_class is None:\n",
    "        raise ValueError(\"predicted and true must be not None\")\n",
    "    \n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    acc = accuracy.compute(predictions=predicted_class, references=true_class, )\n",
    "    \n",
    "    precision = evaluate.load(\"precision\")\n",
    "    prec = precision.compute(predictions=predicted_class, references=true_class, average=None)\n",
    "    prec_weigted = precision.compute(predictions=predicted_class, references=true_class, average=\"macro\")\n",
    "\n",
    "    recall = evaluate.load(\"recall\")\n",
    "    rec = recall.compute(predictions=predicted_class, references=true_class, average=None)\n",
    "    rec_weigted = recall.compute(predictions=predicted_class, references=true_class, average=\"macro\")\n",
    "\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    calc_f1 = f1.compute(predictions=predicted_class, references=true_class, average=None)\n",
    "    f1_weighted = f1.compute(predictions=predicted_class, references=true_class, average=\"macro\")\n",
    "    return acc, prec, prec_weigted, rec, rec_weigted, calc_f1, f1_weighted\n",
    "    # return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(os.path.join(\"data-imbalance\", \"test-imbalance.csv\"))\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trained on imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96ba57b61cb4e179b5329cb68e81ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:17, 111.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"J1mb0o/semantic-bert-imbalanced-dataset\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"J1mb0o/semantic-bert-imbalanced-dataset\").to(device)\n",
    "\n",
    "predicted_class = []\n",
    "true_class = []\n",
    "\n",
    "for text, label in tqdm.tqdm( zip(test_dataset[\"tweet\"], test_dataset[\"label\"])):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_class.append(predicted_class_id)\n",
    "    true_class.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_unbalanced = model_evaluation(predicted_class=predicted_class, true_class=true_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'accuracy': 0.6425}, {'precision': array([0.57435897, 0.5144357 , 0.74880153])}, {'precision': 0.6125320679778218}, {'recall': array([0.34461538, 0.57562408, 0.78571429])}, {'recall': 0.568651250853894}, {'f1': array([0.43076923, 0.54331254, 0.76681394])}, {'f1': 0.5802985720511494})\n"
     ]
    }
   ],
   "source": [
    "print(results_unbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trained on Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d406feb2254c3c85acbfb3b94804be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:15, 127.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"J1mb0o/semantic-bert-balanced-dataset\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"J1mb0o/semantic-bert-balanced-dataset\").to(device)\n",
    "\n",
    "predicted_class = []\n",
    "true_class = []\n",
    "\n",
    "for text, label in tqdm.tqdm( zip(test_dataset[\"tweet\"], test_dataset[\"label\"])):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_class.append(predicted_class_id)\n",
    "    true_class.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_balanced = model_evaluation(predicted_class=predicted_class, true_class=true_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'accuracy': 0.5785}, {'precision': array([0.37605634, 0.50555556, 0.76129032])}, {'precision': 0.5476340720547899}, {'recall': array([0.82153846, 0.26725404, 0.71227364])}, {'recall': 0.6003553805229055}, {'f1': array([0.51594203, 0.34966378, 0.73596674])}, {'f1': 0.5338575165915098})\n"
     ]
    }
   ],
   "source": [
    "print(results_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral 7b Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt1(text):\n",
    "    return f\"\"\"Your task is to classify a tweet sentiment as positive, negative or neutral only. \n",
    "    \n",
    "    Tweet: {text}\n",
    "    \n",
    "    Just generate the JSON object without explanations. Don't forget to close the JSON object with a curly bracket.\n",
    "    \"\"\"\n",
    "\n",
    "def create_prompt2(text):\n",
    "    return f\"\"\"Your task is to classify a tweet sentiment as positive, negative or neutral only. \n",
    "    Think step by step. \n",
    "    \n",
    "    Tweet: {text}\n",
    "    \n",
    "    Just generate the JSON object without explanations. Don't forget to close the JSON object with a curly bracket.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "import string\n",
    "import json\n",
    "llm = Ollama(model=\"mistral\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on first prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [28:59,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_class = []\n",
    "true_class = []\n",
    "\n",
    "for text, label in tqdm.tqdm( zip(test_dataset[\"tweet\"], test_dataset[\"label\"])):\n",
    "    prompt = create_prompt1(text)\n",
    "    response = llm(prompt)\n",
    "    response = json.loads(response)\n",
    "    predicted_class.append(response[\"sentiment\"])\n",
    "    true_class.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predicted_class:\n",
    "    if i not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = [[\"negative\", \"neutral\", \"positive\"].index(i) for i in predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_prompt1 = model_evaluation(predicted_class=predicted_class_id, true_class=true_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import json\n",
    "# text = test_dataset.sample(1)\n",
    "# print(text[\"tweet\"].values[0], text[\"label\"].values[0])\n",
    "# prompt = create_prompt(text[\"tweet\"].values[0])\n",
    "# response = llm(prompt)\n",
    "# # response = ''.join(ch for ch in response.strip().lower() if ch not in string.punctuation)\n",
    "# response = json.loads(response)\n",
    "# print(response[\"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.608},\n",
       " {'precision': array([0.56743003, 0.46439024, 0.88831615])},\n",
       " {'precision': 0.640045473516827},\n",
       " {'recall': array([0.68615385, 0.6989721 , 0.52012072])},\n",
       " {'recall': 0.6350822234510266},\n",
       " {'f1': array([0.62116992, 0.55803048, 0.65609137])},\n",
       " {'f1': 0.6117639225498076})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_results_prompt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on second prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [29:06,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_class = []\n",
    "true_class = []\n",
    "\n",
    "for text, label in tqdm.tqdm( zip(test_dataset[\"tweet\"], test_dataset[\"label\"])):\n",
    "    prompt = create_prompt2(text)\n",
    "    response = llm(prompt)\n",
    "    response = json.loads(response)\n",
    "    predicted_class.append(response[\"sentiment\"])\n",
    "    true_class.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predicted_class:\n",
    "    if i not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = [[\"negative\", \"neutral\", \"positive\"].index(i) for i in predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_prompt2 = model_evaluation(predicted_class=predicted_class_id, true_class=true_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.611},\n",
       " {'precision': array([0.56377551, 0.46719682, 0.8820598 ])},\n",
       " {'precision': 0.6376773766513402},\n",
       " {'recall': array([0.68      , 0.69016153, 0.53420523])},\n",
       " {'recall': 0.6347889195180875},\n",
       " {'f1': array([0.61645746, 0.55720213, 0.66541353])},\n",
       " {'f1': 0.613024376481984})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_results_prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls_df = pd.DataFrame(columns=[\"Model\", \n",
    "                      \"Accuracy\", \n",
    "                      \"Pre_{Negative}\", \n",
    "                      \"Pre_{Neu}\", \n",
    "                      \"Pre_{Pos}\", \n",
    "                      \"Pre_{Weighted}\", \n",
    "                      \"Rec_{Negative}\", \n",
    "                      \"Rec_{Neu}\", \n",
    "                      \"Rec_{Pos}\", \n",
    "                      \"Rec_{Weighted}\", \n",
    "                      \"F1_{Negative}\", \n",
    "                      \"F1_{Neu}\", \n",
    "                      \"F1_{Pos}\", \n",
    "                      \"F1_{Weighted}\"],\n",
    "                      data=[[\"DistilBERT (unbalanced)\", \n",
    "                              results_unbalanced[0][\"accuracy\"], \n",
    "                              results_unbalanced[1][\"precision\"][0], \n",
    "                              results_unbalanced[1][\"precision\"][1],\n",
    "                              results_unbalanced[1][\"precision\"][2],\n",
    "                              results_unbalanced[2][\"precision\"],\n",
    "                              results_unbalanced[3][\"recall\"][0],\n",
    "                              results_unbalanced[3][\"recall\"][1],\n",
    "                              results_unbalanced[3][\"recall\"][2],\n",
    "                              results_unbalanced[4][\"recall\"], \n",
    "                              results_unbalanced[5][\"f1\"][0],\n",
    "                              results_unbalanced[5][\"f1\"][1],\n",
    "                              results_unbalanced[5][\"f1\"][2],\n",
    "                              results_unbalanced[6][\"f1\"]],\n",
    "                              [\"DistilBERT (balanced)\",\n",
    "                              results_balanced[0][\"accuracy\"],\n",
    "                              results_balanced[1][\"precision\"][0],\n",
    "                              results_balanced[1][\"precision\"][1],\n",
    "                              results_balanced[1][\"precision\"][2],\n",
    "                              results_balanced[2][\"precision\"],\n",
    "                              results_balanced[3][\"recall\"][0],\n",
    "                              results_balanced[3][\"recall\"][1],\n",
    "                              results_balanced[3][\"recall\"][2],\n",
    "                              results_balanced[4][\"recall\"],\n",
    "                              results_balanced[5][\"f1\"][0],\n",
    "                              results_balanced[5][\"f1\"][1],\n",
    "                              results_balanced[5][\"f1\"][2],\n",
    "                              results_balanced[6][\"f1\"]],\n",
    "                              [\"Mistral (prompt 1)\",\n",
    "                              mistral_results_prompt1[0][\"accuracy\"],\n",
    "                              mistral_results_prompt1[1][\"precision\"][0],\n",
    "                              mistral_results_prompt1[1][\"precision\"][1],\n",
    "                              mistral_results_prompt1[1][\"precision\"][2],\n",
    "                              mistral_results_prompt1[2][\"precision\"],\n",
    "                              mistral_results_prompt1[3][\"recall\"][0],\n",
    "                              mistral_results_prompt1[3][\"recall\"][1],\n",
    "                              mistral_results_prompt1[3][\"recall\"][2],\n",
    "                              mistral_results_prompt1[4][\"recall\"],\n",
    "                              mistral_results_prompt1[5][\"f1\"][0],\n",
    "                              mistral_results_prompt1[5][\"f1\"][1],\n",
    "                              mistral_results_prompt1[5][\"f1\"][2],\n",
    "                              mistral_results_prompt1[6][\"f1\"]],\n",
    "                              [\"Mistral (prompt 2)\",\n",
    "                              mistral_results_prompt2[0][\"accuracy\"],\n",
    "                              mistral_results_prompt2[1][\"precision\"][0],\n",
    "                              mistral_results_prompt2[1][\"precision\"][1],\n",
    "                              mistral_results_prompt2[1][\"precision\"][2],\n",
    "                              mistral_results_prompt2[2][\"precision\"],\n",
    "                              mistral_results_prompt2[3][\"recall\"][0],\n",
    "                              mistral_results_prompt2[3][\"recall\"][1],\n",
    "                              mistral_results_prompt2[3][\"recall\"][2],\n",
    "                              mistral_results_prompt2[4][\"recall\"],\n",
    "                              mistral_results_prompt2[5][\"f1\"][0],\n",
    "                              mistral_results_prompt2[5][\"f1\"][1],\n",
    "                              mistral_results_prompt2[5][\"f1\"][2],\n",
    "                              mistral_results_prompt2[6][\"f1\"]],\n",
    "                                                  \n",
    "                        ]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls_df.to_csv(\"results-unbalanced.csv\", index=False)\n",
    "resutls_df.to_latex(\"results-unbalanced.tex\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(os.path.join(\"data-balance\", \"test-balance.csv\"))\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trained on imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "975it [00:07, 126.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"J1mb0o/semantic-bert-imbalanced-dataset\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"J1mb0o/semantic-bert-imbalanced-dataset\").to(device)\n",
    "\n",
    "predicted_class = []\n",
    "true_class = []\n",
    "\n",
    "for text, label in tqdm.tqdm( zip(test_dataset[\"tweet\"], test_dataset[\"label\"])):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_class.append(predicted_class_id)\n",
    "    true_class.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_unbalanced = model_evaluation(predicted_class=predicted_class, true_class=true_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'accuracy': 0.5641025641025641}, {'precision': array([0.73202614, 0.43891403, 0.64210526])}, {'precision': 0.6043484780326885}, {'recall': array([0.34461538, 0.59692308, 0.75076923])}, {'recall': 0.564102564102564}, {'f1': array([0.46861925, 0.50586701, 0.69219858])}, {'f1': 0.5555616142545996})\n"
     ]
    }
   ],
   "source": [
    "print(results_unbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trained on Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "975it [00:07, 127.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"J1mb0o/semantic-bert-balanced-dataset\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"J1mb0o/semantic-bert-balanced-dataset\").to(device)\n",
    "\n",
    "predicted_class = []\n",
    "true_class = []\n",
    "\n",
    "for text, label in tqdm.tqdm( zip(test_dataset[\"tweet\"], test_dataset[\"label\"])):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_class.append(predicted_class_id)\n",
    "    true_class.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_balanced = model_evaluation(predicted_class=predicted_class, true_class=true_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'accuracy': 0.598974358974359}, {'precision': array([0.57543103, 0.53409091, 0.66567164])}, {'precision': 0.5917311951215708}, {'recall': array([0.82153846, 0.28923077, 0.68615385])}, {'recall': 0.5989743589743589}, {'f1': array([0.67680608, 0.3752495 , 0.67575758])}, {'f1': 0.5759377201352566})\n"
     ]
    }
   ],
   "source": [
    "print(results_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral 7b Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt1(text):\n",
    "    return f\"\"\"Your task is to classify a tweet sentiment as positive, negative or neutral only. \n",
    "    \n",
    "    Tweet: {text}\n",
    "    \n",
    "    Just generate the JSON object without explanations. Don't forget to close the JSON object with a curly bracket.\n",
    "    \"\"\"\n",
    "\n",
    "def create_prompt2(text):\n",
    "    return f\"\"\"Your task is to classify a tweet sentiment as positive, negative or neutral only. \n",
    "    Think step by step. \n",
    "    \n",
    "    Tweet: {text}\n",
    "    \n",
    "    Just generate the JSON object without explanations. Don't forget to close the JSON object with a curly bracket.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "import string\n",
    "import json\n",
    "llm = Ollama(model=\"mistral\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on first prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "975it [13:55,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_class = []\n",
    "true_class = []\n",
    "\n",
    "for text, label in tqdm.tqdm( zip(test_dataset[\"tweet\"], test_dataset[\"label\"])):\n",
    "    prompt = create_prompt1(text)\n",
    "    response = llm(prompt)\n",
    "    response = json.loads(response)\n",
    "    predicted_class.append(response[\"sentiment\"])\n",
    "    true_class.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predicted_class:\n",
    "    if i not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = [[\"negative\", \"neutral\", \"positive\"].index(i) for i in predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_prompt1 = model_evaluation(predicted_class=predicted_class_id, true_class=true_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import json\n",
    "# text = test_dataset.sample(1)\n",
    "# print(text[\"tweet\"].values[0], text[\"label\"].values[0])\n",
    "# prompt = create_prompt(text[\"tweet\"].values[0])\n",
    "# response = llm(prompt)\n",
    "# # response = ''.join(ch for ch in response.strip().lower() if ch not in string.punctuation)\n",
    "# response = json.loads(response)\n",
    "# print(response[\"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.6328205128205128},\n",
       " {'precision': array([0.72875817, 0.48523207, 0.84102564])},\n",
       " {'precision': 0.68500529282361},\n",
       " {'recall': array([0.68615385, 0.70769231, 0.50461538])},\n",
       " {'recall': 0.6328205128205129},\n",
       " {'f1': array([0.70681458, 0.57571965, 0.63076923])},\n",
       " {'f1': 0.6377678201209597})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_results_prompt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on second prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "975it [14:07,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_class = []\n",
    "true_class = []\n",
    "\n",
    "for text, label in tqdm.tqdm( zip(test_dataset[\"tweet\"], test_dataset[\"label\"])):\n",
    "    prompt = create_prompt2(text)\n",
    "    response = llm(prompt)\n",
    "    response = json.loads(response)\n",
    "    predicted_class.append(response[\"sentiment\"])\n",
    "    true_class.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predicted_class:\n",
    "    if i not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = [[\"negative\", \"neutral\", \"positive\"].index(i) for i in predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_prompt2 = model_evaluation(predicted_class=predicted_class_id, true_class=true_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.6369230769230769},\n",
       " {'precision': array([0.72459016, 0.49036403, 0.84236453])},\n",
       " {'precision': 0.6857729072166873},\n",
       " {'recall': array([0.68      , 0.70461538, 0.52615385])},\n",
       " {'recall': 0.6369230769230769},\n",
       " {'f1': array([0.7015873 , 0.57828283, 0.64772727])},\n",
       " {'f1': 0.6425324675324676})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_results_prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls_df = pd.DataFrame(columns=[\"Model\", \n",
    "                      \"Accuracy\", \n",
    "                      \"Pre_{Negative}\", \n",
    "                      \"Pre_{Neu}\", \n",
    "                      \"Pre_{Pos}\", \n",
    "                      \"Pre_{Weighted}\", \n",
    "                      \"Rec_{Negative}\", \n",
    "                      \"Rec_{Neu}\", \n",
    "                      \"Rec_{Pos}\", \n",
    "                      \"Rec_{Weighted}\", \n",
    "                      \"F1_{Negative}\", \n",
    "                      \"F1_{Neu}\", \n",
    "                      \"F1_{Pos}\", \n",
    "                      \"F1_{Weighted}\"],\n",
    "                      data=[[\"DistilBERT (unbalanced)\", \n",
    "                              results_unbalanced[0][\"accuracy\"], \n",
    "                              results_unbalanced[1][\"precision\"][0], \n",
    "                              results_unbalanced[1][\"precision\"][1],\n",
    "                              results_unbalanced[1][\"precision\"][2],\n",
    "                              results_unbalanced[2][\"precision\"],\n",
    "                              results_unbalanced[3][\"recall\"][0],\n",
    "                              results_unbalanced[3][\"recall\"][1],\n",
    "                              results_unbalanced[3][\"recall\"][2],\n",
    "                              results_unbalanced[4][\"recall\"], \n",
    "                              results_unbalanced[5][\"f1\"][0],\n",
    "                              results_unbalanced[5][\"f1\"][1],\n",
    "                              results_unbalanced[5][\"f1\"][2],\n",
    "                              results_unbalanced[6][\"f1\"]],\n",
    "                              [\"DistilBERT (balanced)\",\n",
    "                              results_balanced[0][\"accuracy\"],\n",
    "                              results_balanced[1][\"precision\"][0],\n",
    "                              results_balanced[1][\"precision\"][1],\n",
    "                              results_balanced[1][\"precision\"][2],\n",
    "                              results_balanced[2][\"precision\"],\n",
    "                              results_balanced[3][\"recall\"][0],\n",
    "                              results_balanced[3][\"recall\"][1],\n",
    "                              results_balanced[3][\"recall\"][2],\n",
    "                              results_balanced[4][\"recall\"],\n",
    "                              results_balanced[5][\"f1\"][0],\n",
    "                              results_balanced[5][\"f1\"][1],\n",
    "                              results_balanced[5][\"f1\"][2],\n",
    "                              results_balanced[6][\"f1\"]],\n",
    "                              [\"Mistral (prompt 1)\",\n",
    "                              mistral_results_prompt1[0][\"accuracy\"],\n",
    "                              mistral_results_prompt1[1][\"precision\"][0],\n",
    "                              mistral_results_prompt1[1][\"precision\"][1],\n",
    "                              mistral_results_prompt1[1][\"precision\"][2],\n",
    "                              mistral_results_prompt1[2][\"precision\"],\n",
    "                              mistral_results_prompt1[3][\"recall\"][0],\n",
    "                              mistral_results_prompt1[3][\"recall\"][1],\n",
    "                              mistral_results_prompt1[3][\"recall\"][2],\n",
    "                              mistral_results_prompt1[4][\"recall\"],\n",
    "                              mistral_results_prompt1[5][\"f1\"][0],\n",
    "                              mistral_results_prompt1[5][\"f1\"][1],\n",
    "                              mistral_results_prompt1[5][\"f1\"][2],\n",
    "                              mistral_results_prompt1[6][\"f1\"]],\n",
    "                              [\"Mistral (prompt 2)\",\n",
    "                              mistral_results_prompt2[0][\"accuracy\"],\n",
    "                              mistral_results_prompt2[1][\"precision\"][0],\n",
    "                              mistral_results_prompt2[1][\"precision\"][1],\n",
    "                              mistral_results_prompt2[1][\"precision\"][2],\n",
    "                              mistral_results_prompt2[2][\"precision\"],\n",
    "                              mistral_results_prompt2[3][\"recall\"][0],\n",
    "                              mistral_results_prompt2[3][\"recall\"][1],\n",
    "                              mistral_results_prompt2[3][\"recall\"][2],\n",
    "                              mistral_results_prompt2[4][\"recall\"],\n",
    "                              mistral_results_prompt2[5][\"f1\"][0],\n",
    "                              mistral_results_prompt2[5][\"f1\"][1],\n",
    "                              mistral_results_prompt2[5][\"f1\"][2],\n",
    "                              mistral_results_prompt2[6][\"f1\"]],\n",
    "                                                  \n",
    "                        ]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls_df.to_csv(\"results-balanced.csv\", index=False)\n",
    "resutls_df.to_latex(\"results-balanced.tex\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "rforest = pickle.load(open(\"random_forest_model.sav\", \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
